{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b393ca",
   "metadata": {},
   "source": [
    "(a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "Resource requirements: ≤ 30 minutes (no GPUs), ≤ 30 GB RAM\n",
    "\n",
    "Hint: You should be able to get under 2 minutes for BPE training using multiprocessing during pre-tokenization and the following two facts:   \n",
    "(a) The <|endoftext|> token delimits documents in the data files.   \n",
    "(b) The <|endoftext|> token is handled as a special case before the BPE merges are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f22460",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "**Time taken:** 25m:50.901s  \n",
    "**Peak Memory:** 15G  \n",
    "**Longest token:** (15) b' accomplishment'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a88b83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(b) Profile your code. What part of the tokenizer training process takes the most time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f701b8",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "`BPETokenizer._compute_idx_freq` takes 46% of the total time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
